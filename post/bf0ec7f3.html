<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="书本总结,"><meta name="description" content="理论基础：Kearns提出“强可学习”与“弱可学习”的概念，“强可学习”：一个概念，存在一个多项式的学习算法能够学习它且正确率很高；&amp;quot;弱可学习&amp;quot;：一个概念，存在一个多项式的学习算法能够学习它且正确率仅比随机猜测略高。Schapire后来证明&amp;quot;强可学习&amp;quot;与&amp;quot;弱可学习&amp;quot;等价。显然&amp;quot;&amp;quot;弱可学习&amp;quot;&amp;quot;算法通常"><meta name="keywords" content="书本总结"><meta property="og:type" content="article"><meta property="og:title" content="第七章 集成学习"><meta property="og:url" content="http://tinymd.github.io/post/bf0ec7f3.html"><meta property="og:site_name" content="Tiny‘s blog"><meta property="og:description" content="理论基础：Kearns提出“强可学习”与“弱可学习”的概念，“强可学习”：一个概念，存在一个多项式的学习算法能够学习它且正确率很高；&amp;quot;弱可学习&amp;quot;：一个概念，存在一个多项式的学习算法能够学习它且正确率仅比随机猜测略高。Schapire后来证明&amp;quot;强可学习&amp;quot;与&amp;quot;弱可学习&amp;quot;等价。显然&amp;quot;&amp;quot;弱可学习&amp;quot;&amp;quot;算法通常"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2020-03-15T11:34:13.952Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="第七章 集成学习"><meta name="twitter:description" content="理论基础：Kearns提出“强可学习”与“弱可学习”的概念，“强可学习”：一个概念，存在一个多项式的学习算法能够学习它且正确率很高；&amp;quot;弱可学习&amp;quot;：一个概念，存在一个多项式的学习算法能够学习它且正确率仅比随机猜测略高。Schapire后来证明&amp;quot;强可学习&amp;quot;与&amp;quot;弱可学习&amp;quot;等价。显然&amp;quot;&amp;quot;弱可学习&amp;quot;&amp;quot;算法通常"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://tinymd.github.io/post/bf0ec7f3.html"><title>第七章 集成学习 | Tiny‘s blog</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Tiny‘s blog</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://tinymd.github.io/post/bf0ec7f3.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="tiny"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Tiny‘s blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">第七章 集成学习</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-11T12:10:01+08:00">2020-03-11</time></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>理论基础：Kearns提出“强可学习”与“弱可学习”的概念，“强可学习”：一个概念，存在一个多项式的学习算法能够学习它且正确率很高；&quot;弱可学习&quot;：一个概念，存在一个多项式的学习算法能够学习它且正确率仅比随机猜测略高。Schapire后来证明&quot;强可学习&quot;与&quot;弱可学习&quot;等价。</p><p>显然&quot;&quot;弱可学习&quot;&quot;算法通常容易得多，那么在此基础之上如何将其转换为&quot;强可学习&quot;算法，这就是提升算法研究的事情，又称集成学习。</p><p>集成学习是组合一系列弱分类器构成一个强分类器。根据这些弱分类器的生成方式，目前集成学习大致分为两类，<strong>第一类</strong>是个体学习器间存在强依赖关系，必须串行顺序生成，代表是Boosting族算法，工作机制类似通过改变数据的概率分布（即数据的权值分布）使得先前基学习器做错的样本在后续得到更多关注以学得不同的弱分类器，其中最具代表的是AdaBoost算法。<strong>第二类</strong>是个体学习器间不存在强依赖关系，可以并行生成，代表是Bagging和随机森林。</p></blockquote><p>第一类Boosting算法主要参照《统计学习方法》，第二类Bagging和随机森林主要参考周志华《西瓜书》。</p><h1 id="adaboost算法">1. AdaBoost算法</h1><h2 id="算法描述">1.1 算法描述</h2><p>输入:训练数据集<span class="math inline">\(T=\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\}\)</span>，<span class="math inline">\(y_i\in Y=\{-1,+1\}\)</span>；弱学习算法（如给定参数的决策树）。</p><p>输出：最终分类器<span class="math inline">\(G(x)\)</span>。</p><ol type="1"><li>初始化训练数据的权值分布，所有数据的权值相等，等于<span class="math inline">\(\frac{1}{N}\)</span>。</li><li>对<span class="math inline">\(m=1,2,\ldots,M\)</span>:<ol type="1"><li>使用具有权值分布<span class="math inline">\(D_m\)</span>的训练数据集学习，根据确定的学习算法得到基本分类器<span class="math inline">\(G_m(x)\)</span>，目标是分类误差率最小。</li><li>计算分类器<span class="math inline">\(G_m(x)\)</span>在训练集上的分类误差率。<span class="math inline">\(e_m=\sum_{i=1}^N\omega_{m,i}I(G_m(x_i)\neq y_i)\)</span>.</li><li>计算<span class="math inline">\(G_m(x)\)</span>的系数<span class="math inline">\(\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}\)</span>.分类器的权重与错误率成反比</li><li>更新权值分布<span class="math inline">\(\omega_{m+1,i}=\omega_{m,i}\frac{exp(-\alpha_my_iG_m(x))}{Z_m}\)</span>，<span class="math inline">\(Z_m\)</span>是归一化因子，对于正确分类的样本<span class="math inline">\(\omega_{m+1,i}=\omega_{m,i}\frac{e^{-\alpha_m}}{Z_m}\)</span>；对于错误分类的样本<span class="math inline">\(\omega_{m+1,i}=\omega_{m,i}\frac{e^{\alpha_m}}{Z_m}\)</span>。误分类样本的权值得以放大。</li></ol></li><li>得到最终分类器 <span class="math inline">\(G(x)=sign(\sum_{m=1}^M\alpha_mG_m(x))\)</span></li></ol><h2 id="前向分步算法与adaboost">1.2 前向分步算法与Adaboost</h2><blockquote><p>Adaboost可从另一个角度理解。首先介绍前向分步算法，然后推导为何AdaBoost是前向分步算法的一个特例。</p></blockquote><h3 id="前向分步算法">1.2.1 前向分步算法</h3><p>加法模型: <span class="math inline">\(f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)\)</span>。其中<span class="math inline">\(b(x;\gamma_m)\)</span>为基函数。加法模型的定义是多个模型的加权和(这些模型来自同一模型假设空间，但不同参数)。</p><p>给定训练数据和损失函数<span class="math inline">\(L(y,f(x))\)</span>的条件下，学习加法模型成为损失函数最小化问题：<span class="math inline">\(\min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x;\gamma_m))\)</span>。</p><p>由上式可知，不同<span class="math inline">\(m\)</span>的<span class="math inline">\(\beta_m,\gamma_m\)</span>互相影响，相同m的<span class="math inline">\(\beta_m,\gamma_m\)</span>也相互影响，因此上述问题是一个复杂的多变量优化问题。前向分步算法对于这种问题的解决办法的核心思想就在分步这两个字。思路：每一步只学习一个基函数及其参数，逐步逼近地最小化损失函数。这样方法可降低优化复杂度并取得次优解。</p><p><strong>算法描述：</strong></p><p>输入:训练数据集<span class="math inline">\(T=\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\}\)</span>，损失函数<span class="math inline">\(L(y;f(x))\)</span>;基本函数集<span class="math inline">\(\{b(x;\gamma)\}\)</span>；</p><p>输出：加法模型<span class="math inline">\(f(x)\)</span>。</p><ol type="1"><li>初始化<span class="math inline">\(f_0(x)=0\)</span>；</li><li>对<span class="math inline">\(m=1,2,\ldots,M\)</span><ol type="1"><li>学习该轮的基函数及其参数。<span class="math inline">\((\beta_m,\gamma_m)=\arg\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))\)</span></li><li>更新加法模型<span class="math inline">\(f_m(x)=f_{m-1}x+\beta_mb(x;\gamma_m)\)</span></li></ol></li><li>M轮之后，得到加法模型<span class="math inline">\(f(x)=f_M(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)\)</span></li></ol><h3 id="推导">1.2.2 推导</h3><p><strong>定理：</strong>AdaBoost的最终分类器<span class="math inline">\(f(x)=\sum_{m=1}^M\alpha_mG_m(x)\)</span>，是由基本分类器(<span class="math inline">\(G_m(x)\in\{-1,1\}\)</span>)组成的加法模型，是前向分步算法的特例，损失函数是指数函数<span class="math inline">\(L(y,f(x))=exp[-yf(x)]\)</span>。</p><p><strong>证明：</strong></p><p>假设经过m-1轮迭代前向分步算法得到了<span class="math inline">\(f_{m-1}(x)\)</span>:</p><p><span class="math inline">\(f_{m-1}(x)=f_{m-2}(x)+\alpha_{m-1}G_{m-1}(x)\)</span>。</p><p>在第m轮迭代中，最小化损失函数以学得该轮的基本分类器及其参数。</p><p>​ <span class="math inline">\((\alpha_m,G_m(x))=\arg\min_{\alpha,G}\sum_{i=1}^Nexp[-y_i(f_{m-1}(x)+\alpha G(x_i))]\)</span>（1）</p><p>令<span class="math inline">\(\bar{\omega_{m,i}}=exp[-y_if_{m-1}(x)]\)</span>，式(1)转换为式(2)</p><p>​ <span class="math inline">\(\min_{\alpha,G}\sum_{i=1}^N\bar{\omega_{m,i}}exp[-y_i\alpha G(x_i))]\)</span> （2）</p><p>接下来证明使式(2)达到最小的<span class="math inline">\(G_m^*(x)\)</span>和<span class="math inline">\(\alpha_m^*\)</span>就是AdaBoost算法中改变权重分布，最小化分类误差率得到的<span class="math inline">\(G_m(x)\)</span>和<span class="math inline">\(\alpha_m\)</span>。</p><p>化简式(2)中需要被最小化的对象，其中<span class="math inline">\(M_1\)</span>表示正确分类的样本，<span class="math inline">\(M_2\)</span>表示错误分类的样本:</p><p><span class="math inline">\(\sum_{i\in M_1}\bar{\omega_{m,i}}exp(-\alpha)+\sum_{i\in M_2}\bar{\omega_{m,i}}exp(\alpha)\)</span>=<span class="math inline">\(\sum_i\bar{\omega_{m,i}}exp(-\alpha)+\sum_{i\in M_2}\bar{\omega_{m,i}}[exp(\alpha)-exp(-\alpha)]\)</span>=<span class="math inline">\(exp(-\alpha)\sum_i\bar{\omega_{m,i}}+[exp(\alpha)-exp(-\alpha)]\sum_{i\in M_2}\bar{\omega_{m,i}}\)</span> （3）</p><p>首先求<span class="math inline">\(G_m^*(x)\)</span>，显然第一项与<span class="math inline">\(G\)</span>无关，添加一个示性函数，因此<span class="math inline">\(G_m^*(x)=\arg\min_G\sum_i\bar{\omega_{m,i}}I(y_i\ne G(x_i))\)</span>。</p><p>此分类器<span class="math inline">\(G_m^*(x)\)</span>即AdaBoost算法所得的第m轮基本分类器，最小化加权的分类误差。</p><p>然后求<span class="math inline">\(\alpha_m^*\)</span>，对式（3）求导，另导数为0：</p><p><span class="math inline">\(-exp(-\alpha_m^*)\sum_i\bar{\omega_{m,i}}+[exp(\alpha)-exp(-\alpha)]\sum_i\bar{\omega_{m,i}}I(y_i\ne G_m^*(x_i))=0\)</span></p><p>得<span class="math inline">\(\alpha_m^*=\frac{1}{2}\log\frac{1-\frac{\sum_i\bar{\omega_{m,i}}I}{\sum_i\bar{\omega_{m,i}}}}{\frac{\sum_i\bar{\omega_{m,i}}I}{\sum_i\bar{\omega_{m,i}}}}=\frac{1}{2}\log\frac{1-e_m}{e_m}\)</span>。</p><p>其中<span class="math inline">\(e_m=\frac{\sum_i\bar{\omega_{m,i}}I}{\sum_i\bar{\omega_{m,i}}}=\sum_i\omega_{m,i}I(y_i\ne G_m^*(x_i))\)</span>。</p><p>从计算过程可以看成<span class="math inline">\(\alpha_m^*\)</span>和<span class="math inline">\(G_m^*(x)\)</span>依赖于<span class="math inline">\(\omega_{m,i}\)</span>，<span class="math inline">\(\omega_{m,i}\)</span>是归一化的<span class="math inline">\(\bar{\omega_{m,i}}\)</span>，因此可将其看成权重。研究<span class="math inline">\(\bar{\omega_{m,i}}\)</span>:</p><p><span class="math inline">\(\bar{\omega_{m,i}}=exp[-y_if_{m-1}(x)]\)</span>，<span class="math inline">\(f_m(x)=f_{m-1}(x)+\alpha_mG_m(x)\)</span>，</p><p>因此初始化时<span class="math inline">\(f_0(x)=0\)</span>，<span class="math inline">\(\bar{\omega_{1,i}}=1\)</span>，即<span class="math inline">\(\omega_{m,i}=\frac{1}{N}\)</span>。对于第m+1轮的权值更新：<span class="math inline">\(\bar{\omega_{m+1,i}}=\bar{\omega_{m,i}}exp[-y_i\alpha_mG_m(x)]\)</span>。<span class="math inline">\(\omega_{m+1,i}=\frac{\bar{\omega_{m+1,i}}}{\sum_i{\bar{\omega_{m+1,i}}}}=\frac{\bar{\omega_{m,i}}exp[-y_i\alpha_mG_m(x_i)]}{\sum_i\bar{\omega_{m,i}}exp[-y_i\alpha_mG_m(x_i)]}=\frac{\omega_{m,i}\sum_i\bar{\omega_{m,i}}exp[-y_i\alpha_mG_m(x_i)]}{\sum_i\omega_{m,i}\sum_i\bar{\omega_{m,i}}exp[-y_i\alpha_mG_m(x_i)]}\)</span></p><p><span class="math inline">\(=\frac{\omega_{m,i}exp[-y_i\alpha_mG_m(x_i)]}{\sum_i\omega_{m,i}exp[-y_i\alpha_mG_m(x_i)]}\)</span></p><p>这与AdaBoost算法重权值更新完全一致。</p><p><strong>总结</strong>：AdaBoost算法主要注重降低偏差，使得模型最大程度地拟合训练数据。</p><h1 id="bagging">2. Bagging</h1></div><footer class="post-footer"><div class="post-tags"><a href="/tags/书本总结/" rel="tag"># 书本总结</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/post/d029af3d.html" rel="next" title="graphviz绘制流程图"><i class="fa fa-chevron-left"></i> graphviz绘制流程图</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/post/160b7936.html" rel="prev" title="Lecture 1">Lecture 1 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">tiny</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">11</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">5</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#adaboost算法"><span class="nav-text">1. AdaBoost算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#算法描述"><span class="nav-text">1.1 算法描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向分步算法与adaboost"><span class="nav-text">1.2 前向分步算法与Adaboost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向分步算法"><span class="nav-text">1.2.1 前向分步算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推导"><span class="nav-text">1.2.2 推导</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bagging"><span class="nav-text">2. Bagging</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">tiny</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>