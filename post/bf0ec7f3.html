<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="书本总结,"><meta name="description" content="理论基础：Kearns提出“强可学习”与“弱可学习”的概念，“强可学习”：一个概念，存在一个多项式的学习算法能够学习它且正确率很高；&amp;quot;弱可学习&amp;quot;：一个概念，存在一个多项式的学习算法能够学习它且正确率仅比随机猜测略高。Schapire后来证明&amp;quot;强可学习&amp;quot;与&amp;quot;弱可学习&amp;quot;等价。显然&amp;quot;&amp;quot;弱可学习&amp;quot;&amp;quot;算法通常"><meta name="keywords" content="书本总结"><meta property="og:type" content="article"><meta property="og:title" content="第七章 集成学习"><meta property="og:url" content="http://tinymd.github.io/post/bf0ec7f3.html"><meta property="og:site_name" content="Tiny‘s blog"><meta property="og:description" content="理论基础：Kearns提出“强可学习”与“弱可学习”的概念，“强可学习”：一个概念，存在一个多项式的学习算法能够学习它且正确率很高；&amp;quot;弱可学习&amp;quot;：一个概念，存在一个多项式的学习算法能够学习它且正确率仅比随机猜测略高。Schapire后来证明&amp;quot;强可学习&amp;quot;与&amp;quot;弱可学习&amp;quot;等价。显然&amp;quot;&amp;quot;弱可学习&amp;quot;&amp;quot;算法通常"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2020-03-14T09:26:15.498Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="第七章 集成学习"><meta name="twitter:description" content="理论基础：Kearns提出“强可学习”与“弱可学习”的概念，“强可学习”：一个概念，存在一个多项式的学习算法能够学习它且正确率很高；&amp;quot;弱可学习&amp;quot;：一个概念，存在一个多项式的学习算法能够学习它且正确率仅比随机猜测略高。Schapire后来证明&amp;quot;强可学习&amp;quot;与&amp;quot;弱可学习&amp;quot;等价。显然&amp;quot;&amp;quot;弱可学习&amp;quot;&amp;quot;算法通常"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://tinymd.github.io/post/bf0ec7f3.html"><title>第七章 集成学习 | Tiny‘s blog</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Tiny‘s blog</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://tinymd.github.io/post/bf0ec7f3.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="tiny"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Tiny‘s blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">第七章 集成学习</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-11T12:10:01+08:00">2020-03-11</time></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>理论基础：Kearns提出“强可学习”与“弱可学习”的概念，“强可学习”：一个概念，存在一个多项式的学习算法能够学习它且正确率很高；&quot;弱可学习&quot;：一个概念，存在一个多项式的学习算法能够学习它且正确率仅比随机猜测略高。Schapire后来证明&quot;强可学习&quot;与&quot;弱可学习&quot;等价。</p><p>显然&quot;&quot;弱可学习&quot;&quot;算法通常容易得多，那么在此基础之上如何将其转换为&quot;强可学习&quot;算法，这就是提升算法研究的事情。</p><p>提升算法是组合一系列弱分类器构成一个强分类器。大多数提升方法都是通过改变数据的概率分布（即数据的权值分布）学得不同的弱分类器。</p><p>这种方法得到强分类器的有两个研究问题：1. 如何改变数据的权重 2. 如何组合弱分类器。</p></blockquote><p>首先介绍具有代表性的AdaBoost算法的做法。</p><h1 id="adaboost算法">1. AdaBoost算法</h1><h2 id="算法描述">1.1 算法描述</h2><p>输入:训练数据集<span class="math inline">\(T=\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\}\)</span>，<span class="math inline">\(y_i\in Y=\{-1,+1\}\)</span>；弱学习算法（如给定参数的决策树）。</p><p>输出：最终分类器<span class="math inline">\(G(x)\)</span>。</p><ol type="1"><li>初始化训练数据的权值分布，所有数据的权值相等，等于<span class="math inline">\(\frac{1}{N}\)</span>。</li><li>对<span class="math inline">\(m=1,2,\ldots,M\)</span>:<ol type="1"><li>使用具有权值分布<span class="math inline">\(D_m\)</span>的训练数据集学习，根据确定的学习算法得到基本分类器<span class="math inline">\(G_m(x)\)</span>，目标是分类误差率最小。</li><li>计算分类器<span class="math inline">\(G_m(x)\)</span>在训练集上的分类误差率。<span class="math inline">\(e_m=\sum_{i=1}^N\omega_{m,i}I(G_m(x_i)\neq y_i)\)</span>.</li><li>计算<span class="math inline">\(G_m(x)\)</span>的系数<span class="math inline">\(\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}\)</span>.分类器的权重与错误率成反比</li><li>更新权值分布<span class="math inline">\(\omega_{m+1,i}=\omega_{m,i}\frac{exp(-\alpha_my_iG_m(x))}{Z_m}\)</span>，<span class="math inline">\(Z_m\)</span>是归一化因子，对于正确分类的样本<span class="math inline">\(\omega_{m+1,i}=\omega_{m,i}\frac{e^{-\alpha_m}}{Z_m}\)</span>；对于错误分类的样本<span class="math inline">\(\omega_{m+1,i}=\omega_{m,i}\frac{e^{\alpha_m}}{Z_m}\)</span>。误分类样本的权值得以放大。</li></ol></li><li>得到最终分类器 <span class="math inline">\(G(x)=sign(\sum_{m=1}^M\alpha_mG_m(x))\)</span></li></ol><h2 id="前向分步算法与adaboost">1.2 前向分步算法与Adaboost</h2><blockquote><p>Adaboost可从另一个角度理解。首先介绍前向分步算法，然后推导为何AdaBoost是前向分步算法的一个特例。</p></blockquote><h2 id="前向分步算法">1.2.1、 前向分步算法</h2><p>加法模型: <span class="math inline">\(f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)\)</span>。其中<span class="math inline">\(b(x;\gamma_m)\)</span>为基函数。加法模型的定义是多个模型的加权和(这些模型来自同一模型假设空间，但不同参数)。</p><p>给定训练数据和损失函数<span class="math inline">\(L(y,f(x))\)</span>的条件下，学习加法模型成为损失函数最小化问题：<span class="math inline">\(\min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x;\gamma_m))\)</span>。</p><p>由上式可知，不同<span class="math inline">\(m\)</span>的<span class="math inline">\(\beta_m,\gamma_m\)</span>互相影响，相同m的<span class="math inline">\(\beta_m,\gamma_m\)</span>也相互影响，因此上述问题是一个复杂的多变量优化问题。前向分步算法对于这种问题的解决办法的核心思想就在分步这两个字。思路：每一步只学习一个基函数及其参数，逐步逼近地最小化损失函数。这样方法可降低优化复杂度并取得次优解。</p><p><strong>算法描述：</strong></p><p>输入:训练数据集<span class="math inline">\(T=\{(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)\}\)</span>，损失函数<span class="math inline">\(L(y;f(x))\)</span>;基本函数集<span class="math inline">\(\{b(x;\gamma)\}\)</span>；</p><p>输出：加法模型<span class="math inline">\(f(x)\)</span>。</p><ol type="1"><li>初始化<span class="math inline">\(f_0(x)=0\)</span>；</li><li>对<span class="math inline">\(m=1,2,\ldots,M\)</span><ol type="1"><li>学习该轮的基函数及其参数。<span class="math inline">\((\beta_m,\gamma_m)=\arg\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))\)</span><ol start="2" type="1"><li>更新加法模型<span class="math inline">\(f_m(x)=f_{m-1}x+\beta_mb(x;\gamma_m)\)</span></li></ol></li></ol></li><li>M轮之后，得到加法模型<span class="math inline">\(f(x)=f_M(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)\)</span></li></ol><h2 id="推导">1.2.2 推导</h2><p><font color="red">定理</font></p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/书本总结/" rel="tag"># 书本总结</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/post/d029af3d.html" rel="next" title="graphviz绘制流程图"><i class="fa fa-chevron-left"></i> graphviz绘制流程图</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">tiny</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">5</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#adaboost算法"><span class="nav-text">1. AdaBoost算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#算法描述"><span class="nav-text">1.1 算法描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向分步算法与adaboost"><span class="nav-text">1.2 前向分步算法与Adaboost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向分步算法"><span class="nav-text">1.2.1、 前向分步算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#推导"><span class="nav-text">1.2.2 推导</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">tiny</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>