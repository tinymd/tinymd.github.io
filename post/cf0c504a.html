<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="书本总结,机器学习,"><meta name="description" content="第三章 分类上一章介绍了回归任务的大致流程，接下来我们看分类任务怎么做1、MNIST数据集我们将使用MNIST数据集，该数据集是一组70,000张由中学生和美国人口普查局员工手写的数字小图像0-9。每张图像是28*28=784个像素，即每张图像有784个特征，每个特征代表了一个像素的灰度，从0（白色）到255（黑色），且所有图像都被标记了它代表的数字。sklearn提供了相关函数来下载流行的数据集"><meta name="keywords" content="书本总结,机器学习"><meta property="og:type" content="article"><meta property="og:title" content="Hands-on Machine Learning第三章"><meta property="og:url" content="http://tinymd.github.io/post/cf0c504a.html"><meta property="og:site_name" content="Tiny‘s blog"><meta property="og:description" content="第三章 分类上一章介绍了回归任务的大致流程，接下来我们看分类任务怎么做1、MNIST数据集我们将使用MNIST数据集，该数据集是一组70,000张由中学生和美国人口普查局员工手写的数字小图像0-9。每张图像是28*28=784个像素，即每张图像有784个特征，每个特征代表了一个像素的灰度，从0（白色）到255（黑色），且所有图像都被标记了它代表的数字。sklearn提供了相关函数来下载流行的数据集"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/3_1.png"><meta property="og:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/3_2.png"><meta property="og:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/3_3.png"><meta property="og:updated_time" content="2020-02-26T12:14:46.460Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Hands-on Machine Learning第三章"><meta name="twitter:description" content="第三章 分类上一章介绍了回归任务的大致流程，接下来我们看分类任务怎么做1、MNIST数据集我们将使用MNIST数据集，该数据集是一组70,000张由中学生和美国人口普查局员工手写的数字小图像0-9。每张图像是28*28=784个像素，即每张图像有784个特征，每个特征代表了一个像素的灰度，从0（白色）到255（黑色），且所有图像都被标记了它代表的数字。sklearn提供了相关函数来下载流行的数据集"><meta name="twitter:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/3_1.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://tinymd.github.io/post/cf0c504a.html"><title>Hands-on Machine Learning第三章 | Tiny‘s blog</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Tiny‘s blog</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://tinymd.github.io/post/cf0c504a.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="tiny"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Tiny‘s blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Hands-on Machine Learning第三章</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-26T09:54:25+08:00">2020-02-26</time></span></div></header><div class="post-body" itemprop="articleBody"><a id="more"></a><h1 id="第三章-分类"><strong>第三章</strong> 分类</h1><blockquote><p>上一章介绍了回归任务的大致流程，接下来我们看分类任务怎么做</p></blockquote><h1 id="mnist数据集">1、MNIST数据集</h1><p>我们将使用MNIST数据集，该数据集是一组70,000张由中学生和美国人口普查局员工手写的数字小图像0-9。每张图像是28*28=784个像素，即每张图像有784个特征，每个特征代表了一个像素的灰度，从0（白色）到255（黑色），且所有图像都被标记了它代表的数字。</p><p>sklearn提供了相关函数来下载流行的数据集，openml.org是用于机器学习数据和实验的公共库，每个人都可以上传开放的数据集。具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_openml</span><br><span class="line">mnist = fetch_openml(&apos;mnist_784&apos;, version=1)</span><br></pre></td></tr></table></figure><p>由sklearn加载返回的数据集一般都是相似的字典结构，keys包含：</p><ol type="1"><li>'DESCR':数据集的文本介绍</li><li>'data': 数据，每行代表一个实例，每列代表一个特征</li><li>'target':是一数组，包含了所有数据对应的标记</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, y = mnist[&apos;data&apos;], mnist[&apos;target&apos;]</span><br></pre></td></tr></table></figure><p>抽取第一行数据，查看一下它的图像，只需要reshape为(28，28)，用plt.imshow()函数，灰度图像数字到颜色的映射可使用'binary'。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">some_digit = X[0]</span><br><span class="line">some_digit_image = some_digit.reshape(28,28)</span><br><span class="line">plt.imshow(some_digit_image, cmap=&apos;binary&apos;)</span><br><span class="line">plt.axis(&apos;off&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>再查看一下<code>y[0]</code>，验证一下图像与标签是否匹配。返回'5'，注意到图像与标签匹配，但标签是由字符串组成的，一般我们倾向于用数字表示标签，所以将其强制转换为int。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = y.astype(np.uint8)</span><br></pre></td></tr></table></figure><p>在我们进行更多操作前，我们应该划分训练集和测试集，显然这里需要分层抽样，但sklearn已经提前为我们划分好了，我们只需要取前60000个为训练集即可，具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]</span><br></pre></td></tr></table></figure><h1 id="训练一个二分类器">2、训练一个二分类器</h1><p>我们简化这个问题，先尝试训练一个二分类器识别数字5，先创建二分类任务的标签：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_5 = (y_train == 5)</span><br><span class="line">y_test_5 = (y_test == 5)</span><br></pre></td></tr></table></figure><p>先使用SVM的随机梯度下降分类器，sklearn中的SGDClassifier。具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import SGDClassifier</span><br><span class="line"></span><br><span class="line">sgd_clf = SGDClassifier(random_state = 42)</span><br><span class="line">sgd_clf.fit(X_train, y_train_5)</span><br><span class="line">sgd_clf.predit([some_digit])</span><br></pre></td></tr></table></figure><h1 id="性能衡量">3、 性能衡量</h1><p>评估分类器通常比评估回归器要复杂得多，有许多性能指标可用。</p><h2 id="交叉验证评估准确率">3.1、交叉验证评估准确率</h2><p>用上一章使用的sklearn自带的交叉验证函数cross_val_score()，输入选定的模型，训练数据和label，k折的具体数字和评估标准，具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line"></span><br><span class="line">cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=&apos;accuracy&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>当然交叉验证这一功能你也可以自己实现，利用提供的StraifiedKFold函数，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; from sklearn.model_selection import StratifiedKFold</span><br><span class="line">&gt; from sklearn.base import clone</span><br><span class="line">&gt; skfold = StratifiedKFold(n_splits=3, random_state=42)</span><br><span class="line">&gt; </span><br><span class="line">&gt; for train_index, test_index in skfold.split(X_train, y_train_5):</span><br><span class="line">&gt; 	clone_clf = clone(sgd_clf)</span><br><span class="line">&gt; 	X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]</span><br><span class="line">&gt; 	y_train_fold, y_test_fold = y_train_5[train_index], y_train_5[test_index]</span><br><span class="line">&gt; 	clone_clf.fit(X_train_fold, y_train_fold)</span><br><span class="line">&gt; 	y_pred = clone_clf.predict(X_test_fold)</span><br><span class="line">&gt; 	n_correct = sum(y_pred == y_test_fold)</span><br><span class="line">&gt; 	print(n_correct/len(y_pred))</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p>得到交叉验证的准确率为[0.96355, 0.93795, 0.95615]，准确率很高，但这真的能说明分类器的效果吗？我们写一个分类器：所有数字都认为不是5。代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.base import BaseEstimator</span><br><span class="line"></span><br><span class="line">class Never5Classifier(BaseEstimator):</span><br><span class="line">	def fit(self,X,y=None):</span><br><span class="line">		pass</span><br><span class="line">	def predict(self,X):</span><br><span class="line">		return np.zeros((len(X),1), dtype=bool)</span><br></pre></td></tr></table></figure><p>同样运用交叉验证看这个非5分类器的准确率：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">never_5_clf = Never5Classifier()</span><br><span class="line">cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=&apos;accuracy&apos;)</span><br></pre></td></tr></table></figure><p>得到交叉验证的准确率为[0.91125, 0.90855, 0.90915]，不难猜出因为不超过10%的数据是5，从这两个相近的准确率数值可以看出准确率为什么通常不是分类器中习惯使用的分类性能评价指标，尤其是在处理偏斜数据集(某一类别数据明显多于其他)的时候。</p><h2 id="混淆矩阵">3.2、混淆矩阵</h2><p>更好的分类器性能评价指标是混淆矩阵，基本思想是计算A类被分成B类的次数。同样这里探讨误分类也是针对经过训练的分类器，因此我们同样需要需要训练集和验证集来完成这一计算。且最大化数据的利用率，同样可以采用K折的办法。</p><p><strong>这里用K折的办法来混合统计有个前提：假定(k-1)训练数据集够大，使得不同(k-1)数据集学习所得模型几乎与整个训练集训练所得模型相近。</strong></p><p>sklearn提供了cross_val_predict ，它和 cross_val_score的使用方法是一样的，但是它返回的是具体预测值，而不是评分标准。它的运行过程是这样的，假如数据划分为[1,2,3,4,5]份，它先用[1,2,3,4]训练模型，计算出来第5份的目标值，然后用[1,2,3,5]计算出第4份的目标值，直到所有训练数据都有预测值为止，这样你就得到了针对该模型所有训练数据的无污染预测值（此处无污染代表预测模型之前并未见过该数据）。</p><p>大概出于运行时间考虑，书上将其分为了3折，具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_predict</span><br><span class="line"></span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)</span><br></pre></td></tr></table></figure><p>现在你得到了所有训练数据的预测值，可以用confuse_matrix()函数计算混淆矩阵，只需要提供实际值和预测值。代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line"></span><br><span class="line">confuse_matrix(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure><p>得到结果如下：</p><p><img src="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/3_1.png"></p><p>混淆矩阵各个元素代表的含义如下图所示：</p><p><img src="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/3_2.png"></p><p>混淆矩阵中每一行代表一个实际类别，每一列代表一个预测类别，对于二分类器，我们一般会定义一个正类和负类，此处第一行和第一列代表负类，表示不是5。非5的图像被识别为非5的次数为53057，称为True Negative，简写为TN，其他三个结果类似解释，此处的Negative或Positive指的是预测结果是正类还是负类，True或False指的是预测结果是否与实际结果相同。</p><p>我们从混淆矩阵中能够看出很多信息，正类和负类被正确分类的次数，被错误分类的次数，但我们会想要更加精准的衡量标准。这就是接下来讲到的<strong>精准率</strong>和<strong>召回率</strong>。</p><h2 id="精准率和召回率">3.3、精准率和召回率</h2><p>精准率——正类查准的概率，所有分类结果为正类的数据中有多少实际类别为正类的，即预测结果为正类正确的概率。<span class="math inline">\(precision = \frac{TP}{TP+FP}\)</span></p><p>想要得到最佳精准率，我们只需要将确信为正类的一个实例预测为正类即可，其他的都为负类，因此我们不存在FP。precision=1，但显然这个1不能说明什么信息，所以精准率一般都跟另一个评估标准一同使用，这就是召回率。</p><p>召回率——正类查全的概率，所有实际类别为正类的数据中有多少被分类器检测为正类的，即所有正类中被分类正确的比率。<span class="math inline">\(recall = \frac{TP}{TP+FN}\)</span></p><p>sklearn提供了函数方便我们计算精准率与召回率，precision_score(), recall_score()。只需要提供实际值和预测值。具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import precision_score, recall_score</span><br><span class="line"></span><br><span class="line">precision_score(y_train_5, y_train_pred)</span><br><span class="line">recall_score(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure><p>返回结果精准率为0.7290850836596654，召回率为0.7555801512636044。</p><p>当我们要比较两个分类器效果时，有两个指标情况较复杂，因此我们将精准率和召回率通过调和平均的办法结合到一起。<span class="math inline">\(F_1 = \frac{2}{\frac{1}{precision}+\frac{1}{recall}}\)</span>。调和平均相比平均值，会对小值更加敏感，同样举上面那个将确信为正类的一个实例预测为正类，其他的都为负类的例子，此时precision=1,假设正类数据很多(超1000),recall约等于0，但两者平均值依旧有0.5，显然这个数字对这个分类器的分类效果时估计过高的，而调和平均值对小值敏感，F1依旧约等于0。因此若调和平均值F1较高，则precision和recall两者都必须同时较高。同样sklearn提供了计算F1的函数f1_score()，同样只需要提供实际值和预测值。具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import f1_score</span><br><span class="line"></span><br><span class="line">f1_score(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure><p>F1偏向于具有相似精准度和召回率的分类器，而在现实中，有时同时达到高精准度和高召回率是矛盾的，但很多情况下你的着重点会不同：比如为保护孩童的视频过滤，你倾向于高精准率，召回率不会那么重要；针对超市监控的扒手检测，你会更期待高召回率。一般情况下高精准率和高召回率两者不可兼得，这其中涉及到两者的权衡。</p><h2 id="精准率和召回率的权衡">3.4、精准率和召回率的权衡</h2><p>我们从本章使用的分类器SVM来理解这两者之间的权衡，SVM学得一个决策函数来进行分类，SVM计算每个实例的函数取值，如果大于某个阈值则分为正类，反之分为负类。假设所有实例基于此决策函数取值从小到大取值如下：</p><p><img src="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/3_3.png"></p><p>实际情况中，数据是很少能线性可分的，这里也一样，没有哪个阈值可以把正负类彻底分开，当阈值取在正中央时，精准率为4/5，召回率为4/6。提高阈值可以使得精准率为100%，但同时召回率则降低了；降低阈值可以使召回率为100%，但同时精准率降低。</p><p>sklearn并没有让你直接设置这个阈值，但提供函数decison_fuction()你可以访问实例的决策函数取值，具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_scores = sgd_clf.decision_function([some_digit])</span><br></pre></td></tr></table></figure><p>返回结果为2412.53175101。你可以手动设置阈值如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">threshold = 0</span><br><span class="line">y_some_digit_pred = (y_scores &gt; threshold)</span><br></pre></td></tr></table></figure><p></p><p>返回结果为True，这与前面的预测结果一致，sklearn默认的阈值同样为0。当提高阈值到8000时，预测结果为False，这里再一次证实提高阈值降低召回率。（这里的阈值可以理解为决策平面往正类方向平移的距离，显然不管什么时候越大精准率越高，越低召回率越高）</p><p>至于你到底选择阈值为多少？你可以将阈值看成时一个<strong>超参数</strong>—在你定下模型开始学习时需要确定的参数，同样对于超参数的选定，你可以使用交叉验证的办法，根据所有数据在测试时决策函数的取值而定，同样的cross_val_predict()函数可以得到所有数据的决策函数值，只需设置method参数为decision_function。具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,</span><br><span class="line">                             method=&quot;decision_function&quot;)</span><br></pre></td></tr></table></figure><p>有了所有的取值，你可以得到所有阈值可能取值对应的精准率和召回率，利用sklearn提供的precision_recall_curve()。你只需要提供数据的实际类别及所有数据的决策函数值，函数会取所有的划分值即阈值，然后返回对应的精准率和召回率。然后具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import precision_recall_curve</span><br><span class="line"></span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure><p><strong>这里阈值的问题涉及到&quot;SVM&quot;</strong>，我们后面再来补上这里。</p><h2 id="roc曲线">3.5、 ROC曲线</h2><p>ROC曲线是另一个经常被用于二分类的工具，曲线表示TPR vs FPR。TPR(True Positive Rate)就是召回率，FPR就是负类中被分为正类的比率，注意这里的比率都是相对于实际类别而言。</p><h1 id="multiclass分类">4、Multiclass分类</h1><h1 id="错误分析">5、错误分析</h1><h1 id="multilabel分类">6、Multilabel分类</h1><h1 id="multioutput分类">7、Multioutput分类</h1></div><footer class="post-footer"><div class="post-tags"><a href="/tags/书本总结/" rel="tag"># 书本总结</a> <a href="/tags/机器学习/" rel="tag"># 机器学习</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/post/8212f318.html" rel="next" title="Hands-on Machine Learning第二章"><i class="fa fa-chevron-left"></i> Hands-on Machine Learning第二章</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/post/7821492f.html" rel="prev" title="Hands-on Machine Learning第四章">Hands-on Machine Learning第四章 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">tiny</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">5</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第三章-分类"><span class="nav-text">第三章 分类</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mnist数据集"><span class="nav-text">1、MNIST数据集</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#训练一个二分类器"><span class="nav-text">2、训练一个二分类器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#性能衡量"><span class="nav-text">3、 性能衡量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉验证评估准确率"><span class="nav-text">3.1、交叉验证评估准确率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#混淆矩阵"><span class="nav-text">3.2、混淆矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#精准率和召回率"><span class="nav-text">3.3、精准率和召回率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#精准率和召回率的权衡"><span class="nav-text">3.4、精准率和召回率的权衡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#roc曲线"><span class="nav-text">3.5、 ROC曲线</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#multiclass分类"><span class="nav-text">4、Multiclass分类</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#错误分析"><span class="nav-text">5、错误分析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#multilabel分类"><span class="nav-text">6、Multilabel分类</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#multioutput分类"><span class="nav-text">7、Multioutput分类</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">tiny</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>