<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="Hexo, NexT"><meta name="description" content="第四章 模型训练#研究线性回归，用两种不同方式训练模型，一是封闭解法，二是梯度下降。多项式回归，比线性回归更加复杂，更容易过拟合，用learning curve（学习曲线）如何判断过拟合，如何用正则化技术避免过拟合研究两种常用于分类的回归模型: Logestic Regression 和 Softmax Regression。1、线性回归#模型：\(\hat{y} = \theta_0 +\the"><meta property="og:type" content="article"><meta property="og:title" content="tiny的个人博客"><meta property="og:url" content="http://tinymd.github.io/2020/02/17/hands_on_sklearn_tf_ch4/index.html"><meta property="og:site_name" content="tiny的个人博客"><meta property="og:description" content="第四章 模型训练#研究线性回归，用两种不同方式训练模型，一是封闭解法，二是梯度下降。多项式回归，比线性回归更加复杂，更容易过拟合，用learning curve（学习曲线）如何判断过拟合，如何用正则化技术避免过拟合研究两种常用于分类的回归模型: Logestic Regression 和 Softmax Regression。1、线性回归#模型：\(\hat{y} = \theta_0 +\the"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_1.png"><meta property="og:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_2.png"><meta property="og:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_3.png"><meta property="og:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_4.png"><meta property="og:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_5.png"><meta property="og:updated_time" content="2020-02-19T13:01:26.917Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="tiny的个人博客"><meta name="twitter:description" content="第四章 模型训练#研究线性回归，用两种不同方式训练模型，一是封闭解法，二是梯度下降。多项式回归，比线性回归更加复杂，更容易过拟合，用learning curve（学习曲线）如何判断过拟合，如何用正则化技术避免过拟合研究两种常用于分类的回归模型: Logestic Regression 和 Softmax Regression。1、线性回归#模型：\(\hat{y} = \theta_0 +\the"><meta name="twitter:image" content="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_1.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://tinymd.github.io/2020/02/17/hands_on_sklearn_tf_ch4/"><title>| tiny的个人博客</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">tiny的个人博客</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://tinymd.github.io/2020/02/17/hands_on_sklearn_tf_ch4/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="tiny"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="tiny的个人博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-17T11:00:39+08:00">2020-02-17</time></span></div></header><div class="post-body" itemprop="articleBody"><h1><span id="di-si-zhang-mo-xing-xun-lian"><strong>第四章</strong> 模型训练</span><a href="#di-si-zhang-mo-xing-xun-lian" class="header-anchor">#</a></h1><blockquote><ol type="1"><li>研究线性回归，用两种不同方式训练模型，一是封闭解法，二是梯度下降。</li><li>多项式回归，比线性回归更加复杂，更容易过拟合，用learning curve（学习曲线）如何判断过拟合，如何用正则化技术避免过拟合</li><li>研究两种常用于分类的回归模型: Logestic Regression 和 Softmax Regression。</li></ol></blockquote><h1><span id="1-xian-xing-hui-gui">1、线性回归</span><a href="#1-xian-xing-hui-gui" class="header-anchor">#</a></h1><p>模型：<span class="math inline">\(\hat{y} = \theta_0 +\theta_1 x_1+\theta_2 x_2+\ldots+\theta_n x_n\)</span>，用向量表示<span class="math inline">\(\hat{y}=h_\theta(x)=\theta^T x,\theta=(\theta_0,\theta_1,\ldots,\theta_n)^T,x=(1,x_1,\ldots,x_n)^T\)</span>。<span class="math inline">\(h_\theta\)</span>是模型参数为<span class="math inline">\(\theta\)</span>时的假设函数。</p><p>损失函数：同样对于回归任务，我们使用均方差MSE来衡量模型拟合训练数据的效果。<span class="math inline">\(MSE(\theta) = \frac{1}{m}\sum_{i=1}^m(\theta^T x^{(i)}-y^{(i)})^2\)</span>，上标i表示第i个实例。</p><h2><span id="1-1-normal-function">1.1、Normal Function</span><a href="#1-1-normal-function" class="header-anchor">#</a></h2><p>首先看封闭解(closed-form)法：最小化均方差损失函数的解成为标准方程，标准方程为<span class="math inline">\(\hat{\theta}=(X^TX)^{-1}X^Ty\)</span>，具体标准方程的推导可见[https://blog.csdn.net/zhangbaodan1/article/details/81013056]。</p><p>随机生成一些趋势为线性关系的数据，<span class="math inline">\(y = 4+3x_1+高斯噪声\)</span>，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">X = 2 * np.random.rand(100,1)</span><br><span class="line">y = 4 + 3 * X + np.random.randn(100,1)</span><br></pre></td></tr></table></figure><figure><img src="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_1.png" alt="linear looking data"><figcaption>linear looking data</figcaption></figure><p>用normal funtion求解最小化均方差的参数值，使用numpy的inv()函数求矩阵的逆，dot()函数求矩阵函数，具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance</span><br><span class="line">theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br></pre></td></tr></table></figure><p>求解的theta_best为[[4.21509616, 2.77011339]]，在sklearn中使用线性回归很简单，只需要使用LinearRegression这个类，这个类是基于scipy.linalg.lstsq()函数。当然你还可以pinv()计算伪逆，根据奇异值分解求得。具体步骤此处省略，求逆矩阵复杂度为<span class="math inline">\(O(n^2)\)</span>。</p><h2><span id="1-2-ti-du-xia-jiang">1.2、梯度下降</span><a href="#1-2-ti-du-xia-jiang" class="header-anchor">#</a></h2><p>两个主要的挑战：局部最小值和plateau。</p><h3><span id="1-2-1-pi-liang-ti-du-xia-jiang">1.2.1、批量梯度下降</span><a href="#1-2-1-pi-liang-ti-du-xia-jiang" class="header-anchor">#</a></h3><p>为实现梯度下降，你需要计算损失函数关于各个参数<span class="math inline">\(\theta_j\)</span>的<strong>偏导数</strong>，换句话说，你需要知道如果改变<span class="math inline">\(\theta_j\)</span>一点点，损失函数会怎样变化，这就跟你站在一个斜坡上问：如果我面向东边这个坡度是多少？那我面向北边又是多少？</p><p>损失函数关于<span class="math inline">\(\theta_j\)</span>的偏导数:<span class="math inline">\(\frac{\partial}{\partial\theta_j}MSE(\theta)=\frac{2}{m}\sum_{i=1}^{m}(\theta^Tx^{(i)}-y^{(i)})x_j^{(i)}\)</span>。</p><p>则关于<span class="math inline">\(\theta\)</span>的导数为<span class="math inline">\(\frac{\partial}{\partial\theta}MSE(\theta)=\frac{2}{m}X^T(X\theta-y)\)</span>。</p><p>由于梯度是指向增长最快的方向，因此我们下一步的<span class="math inline">\(\theta\)</span>值可修改为(想象成下一步要走向哪里):<span class="math inline">\(\theta^{\prime}=\theta-\eta\frac{\partial}{\partial\theta}MSE(\theta)\)</span>。</p><p>代码实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">eta = 0.1  # learning rate</span><br><span class="line">n_iterations = 1000</span><br><span class="line">m = 100</span><br><span class="line"></span><br><span class="line">theta = np.random.randn(2,1)  # random initialization</span><br><span class="line"></span><br><span class="line">for iteration in range(n_iterations):</span><br><span class="line">    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">    theta = theta - eta * gradients</span><br></pre></td></tr></table></figure><p>此处引入了第一个超参数<span class="math inline">\(\eta\)</span>：设置的学习率<span class="math inline">\(\eta\)</span>不同会导致学习速度学习效果不一样，太小迭代过程很慢，太大会很难收敛，可以使用第二章的格搜索(grid search)来找到一个最佳的学习率<span class="math inline">\(\eta\)</span>，可以设置迭代次数上限来排除那些导致学习过程发散的学习率。</p><p>那么怎么设置迭代上限？如果太小，则在找到最优参数解之前学习过程停止了，如果太大，会浪费很多时间。一般的解决办法是设置一个很大的迭代次数，但新增条件，当梯度小于某个很小的值<span class="math inline">\(\epsilon\)</span>(tolerance)时，模型停止学习，认为已找到最优参数解。</p><h3><span id="1-2-2-sui-ji-ti-du-xia-jiang">1.2.2、随机梯度下降</span><a href="#1-2-2-sui-ji-ti-du-xia-jiang" class="header-anchor">#</a></h3><p>批量梯度下降在每一步使用整个训练集来计算梯度，当训练集很大时，训练速度很慢，而随机梯度下降随机选取一个训练数据计算其损失函数并计算梯度，但这种随机选取一个数据的办法使得总损失可能不会随学习过程严格下降，但总体趋势是下降的，且最后会bounce around，当算法停止时，最后参数近似于最优解，但不是最优解。</p><p>但随机梯度下降还有一优点：因其随机性，它较容易跳出局部最优解。为了使算法能尽可能的接近最优解，通常设置学习率逐渐降低，减慢速度如何设置依旧是门学问。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import SGDRegressor </span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br></pre></td></tr></table></figure><h3><span id="1-2-3-xiao-pi-liang-ti-du-xia-jiang">1.2.3、小批量梯度下降</span><a href="#1-2-3-xiao-pi-liang-ti-du-xia-jiang" class="header-anchor">#</a></h3><p>介于两者之间，每一次随机使用部分训练数据来计算梯度。</p><h1><span id="2-duo-xiang-shi-hui-gui">2、多项式回归</span><a href="#2-duo-xiang-shi-hui-gui" class="header-anchor">#</a></h1><p>如果数据不是线性回归可以拟合好的话，你可以添加特征的幂次作为新特征，并依旧用线性回归的办法拟合，这个过程即为多项式回归。</p><p>先随机产生一些非线性的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = 100</span><br><span class="line">X = 6 * np.random.rand(m, 1) - 3</span><br><span class="line">y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)</span><br></pre></td></tr></table></figure><p>利用sklearn的PolynomialFeatures来添加特征的幂次，PolynomialFeatures同样是个转换器，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">poly_features = PolynomialFeatures(degree=2, include_bias=False)</span><br><span class="line">X_poly = poly_features.fit_transform(X)</span><br></pre></td></tr></table></figure><p>因为sklearn线性回归的模型会自动加上偏置项，因此此处无需加上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_poly, y)</span><br></pre></td></tr></table></figure><p>注意PolynomialFeatures会尝试所有特征的组合，假设你有两个特征a和b，当你设置degree=3时，会添加<span class="math inline">\(a^2,b^2,ab,a^3,a^2b,ab^2,b^3\)</span>。</p><h1><span id="3-xue-xi-qu-xian">3、学习曲线</span><a href="#3-xue-xi-qu-xian" class="header-anchor">#</a></h1><p>多项式较线性回归更加容易过拟合，如何判断过拟合？可以继续用之前介绍的交叉验证的办法，当模型在训练集上误差较小，但在验证集上泛化性能差，则说明模型过拟合。</p><p>现在介绍另一种办法为学习曲线：训练误差和验证误差随着训练数据集增大的变化。编写画学习曲线的代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def plot_learning_curves(model, X, y):</span><br><span class="line">    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)</span><br><span class="line">    train_errors, val_errors = [], []</span><br><span class="line">    for m in range(1, len(X_train)):</span><br><span class="line">        model.fit(X_train[:m], y_train[:m])</span><br><span class="line">        y_train_predict = model.predict(X_train[:m])</span><br><span class="line">        y_val_predict = model.predict(X_val)</span><br><span class="line">        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"></span><br><span class="line">    plt.plot(np.sqrt(train_errors), &quot;r-+&quot;, linewidth=2, label=&quot;train&quot;)</span><br><span class="line">    plt.plot(np.sqrt(val_errors), &quot;b-&quot;, linewidth=3, label=&quot;val&quot;)</span><br></pre></td></tr></table></figure><p>先看线性回归的学习曲线：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lin_reg = LinearRegression()</span><br><span class="line">plot_learning_curves(lin_reg, X, y)</span><br></pre></td></tr></table></figure><p>结果如下图所示：</p><figure><img src="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_2.png" alt="underfit_learn_curve"><figcaption>underfit_learn_curve</figcaption></figure><p>y的取值范围在(0,10)，分析上图：当训练数据为1到2个点时，线性模型能够完美的拟合数据，所以最开始有一段很短的训练误差为0，但是随着新数据的加入，由于数据根本不是线性的且数据含有噪声，模型很快就不能很好地拟合数据了训练误差维持在一个很大的值；一开始模型训练的数据很少，验证误差会极大，随着新数据的加入，验证误差减小，但线性模型不能很好地拟合非线性数据，所以最后验证误差维持在一个很大的值。</p><p>这时典型的欠拟合的学习曲线：训练误差和验证误差都维持在一个很大的值，且两个值很相近。</p><p>然后我们观察10阶多项式的学习曲线：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (&quot;poly_features&quot;, PolynomialFeatures(degree=10, include_bias=False)),</span><br><span class="line">        (&quot;std_scaler&quot;, StandardScaler())    #added by mendy</span><br><span class="line">        (&quot;lin_reg&quot;, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br></pre></td></tr></table></figure><p>结果如下图：</p><figure><img src="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_3.png" alt="overfit_curve"><figcaption>overfit_curve</figcaption></figure><p>这是典型的过拟合，与欠拟合两点不同：</p><ol type="1"><li>此训练误差小于线性模型的训练误差</li><li>模型的训练误差小于验证误差。</li></ol><h1><span id="4-zheng-ze-hua-mo-xing">4、正则化模型</span><a href="#4-zheng-ze-hua-mo-xing" class="header-anchor">#</a></h1><p>对于线性模型，正则化就是控制模型的参数值。注意正则化只在训练过程中被用到，验证过程并不用到，有三种控制参数的办法：Ridge Regression, Lasso Regression, Elastic Net。</p><h2><span id="4-1-ridge-regression">4.1 Ridge Regression</span><a href="#4-1-ridge-regression" class="header-anchor">#</a></h2><p>Ridge：在损失函数项上加上<span class="math inline">\(\alpha\frac{1}{2}\sum_{i=1}^{n}\theta_i^2\)</span>，注意偏置项<span class="math inline">\(\theta_0\)</span>并未正则化，这一正则项强迫模型不仅得良好拟合数据，且模型的参数值得尽可能小。超参数<span class="math inline">\(\alpha\)</span>控制着正则化强度，当<span class="math inline">\(\alpha=0\)</span>时，模型为普通的线性模型，当<span class="math inline">\(\alpha=+inf\)</span>，所有参数值为0，模型预测为所有训练数据的均值。</p><p>sklearn中用封闭式解法求得Ridge Regression的参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Ridge</span><br><span class="line"></span><br><span class="line">ridge_reg = Ridge(alpha=1, solver=&quot;cholesky&quot;, random_state=42)</span><br><span class="line">ridge_reg.fit(X, y)</span><br></pre></td></tr></table></figure><p>采用随机梯度下降：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgd_reg = SGDRegressor(penalty=&quot;l2&quot;, max_iter=1000, tol=1e-3, random_state=42)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br></pre></td></tr></table></figure><p>参数penalty表示正则化选择，l2即l2范式。</p><h2><span id="4-2-lasso-regression">4.2 Lasso Regression</span><a href="#4-2-lasso-regression" class="header-anchor">#</a></h2><p>lasso: 在损失函数项加上<span class="math inline">\(\alpha\sum_{i=1}^{n}|\theta_i|\)</span>，Lasso较Ridge更容易产生稀疏解。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import Lasso</span><br><span class="line">lasso_reg = Lasso(alpha=0.1)</span><br><span class="line">lasso_reg.fit(X, y)</span><br></pre></td></tr></table></figure><p>也可使用SGDRegressor：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgd_reg = SGDRegressor(penalty=&quot;l2&quot;, max_iter=1000, tol=1e-3, random_state=42)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br></pre></td></tr></table></figure><h2><span id="4-3-elastic-net">4.3 Elastic Net</span><a href="#4-3-elastic-net" class="header-anchor">#</a></h2><p>Elastic Net: Ridge和Lasso正则项的线性组合。<span class="math inline">\(r\alpha\sum_{i=1}^{n}|\theta_i|+(1-r)\alpha\frac{1}{2}\sum_{i=1}^{n}\theta_i^2\)</span>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import ElasticNet</span><br><span class="line">elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)</span><br><span class="line">elastic_net.fit(X, y)</span><br><span class="line">elastic_net.predict([[1.5]])</span><br></pre></td></tr></table></figure><p>l1_ratio即r。</p><h2><span id="4-4-ti-qian-ting-zhi">4.4 提前停止</span><a href="#4-4-ti-qian-ting-zhi" class="header-anchor">#</a></h2><p>除开在损失函数中添加限制模型参数的办法外，还有另外一种正则化方法：提前停止。</p><p>在验证误差最小的时候停止训练。</p><p>然而随机梯度下降和小批量梯度下降中由于训练过程是抽取部分实例的损失函数来优化，因此验证误差很可能不会逐渐下降，而是最终会在最小值处波动，我们如果想用提前停止这个办法进行正则化，可以在验证误差在最小值附近一段时间后终止算法，并回归到最小值处。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.base import clone</span><br><span class="line"></span><br><span class="line">poly_scaler = Pipeline([</span><br><span class="line">        (&quot;poly_features&quot;, PolynomialFeatures(degree=90, include_bias=False)),</span><br><span class="line">        (&quot;std_scaler&quot;, StandardScaler())</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">X_train_poly_scaled = poly_scaler.fit_transform(X_train)</span><br><span class="line">X_val_poly_scaled = poly_scaler.transform(X_val)</span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,</span><br><span class="line">                       penalty=None, learning_rate=&quot;constant&quot;, eta0=0.0005, random_state=42)</span><br><span class="line"></span><br><span class="line">minimum_val_error = float(&quot;inf&quot;)</span><br><span class="line">best_epoch = None</span><br><span class="line">best_model = None</span><br><span class="line">for epoch in range(1000):</span><br><span class="line">    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off</span><br><span class="line">    y_val_predict = sgd_reg.predict(X_val_poly_scaled)</span><br><span class="line">    val_error = mean_squared_error(y_val, y_val_predict)</span><br><span class="line">    if val_error &lt; minimum_val_error:</span><br><span class="line">        minimum_val_error = val_error</span><br><span class="line">        best_epoch = epoch</span><br><span class="line">        best_model = clone(sgd_reg)</span><br></pre></td></tr></table></figure><p>warm_start表示fit()继续之前的训练过程，而不是从头开始。</p><h1><span id="5-logistic-regression">5、Logistic Regression</span><a href="#5-logistic-regression" class="header-anchor">#</a></h1><p>Logistic Regression，一种常用于二分类的回归办法，我们先介绍logistic回归办法如何通过评估概率进行分类预测，然后探究其训练方法。最后在一经典数据集-鸢尾花数据集上使用logistic Regression方法分类。</p><h2><span id="5-1-ping-gu-gai-lu">5.1 评估概率</span><a href="#5-1-ping-gu-gai-lu" class="header-anchor">#</a></h2><p>logistic回归计算输入特征的加权总和，与线性回归不同的事，并不直接输出加权和结果，而是利用sigmoid函数<span class="math inline">\(\sigma(t)\)</span>将该值转换为概率。<span class="math inline">\(\hat{p}=h_\theta(x)=\sigma(\theta^Tx)=\frac{1}{1+exp(-\theta^Tx)}\)</span>。</p><p>sigmoid函数图像如下所示：</p><figure><img src="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_4.png" alt="sigmoid"><figcaption>sigmoid</figcaption></figure><p>当<span class="math inline">\(\hat{p}&gt;0.5\)</span>时，预测<span class="math inline">\(\hat{y}=1\)</span>；当<span class="math inline">\(\hat{p}&lt;0.5\)</span>时，预测<span class="math inline">\(\hat{y}=0\)</span>；由上图可知，当加权和<span class="math inline">\(\theta^Tx&gt;0\)</span>时，<span class="math inline">\(\hat{y}=1\)</span>；当<span class="math inline">\(\theta^Tx&lt;0\)</span>时，<span class="math inline">\(\hat{y}=0\)</span>。</p><h2><span id="5-2-sun-shi-han-shu">5.2 损失函数</span><a href="#5-2-sun-shi-han-shu" class="header-anchor">#</a></h2><p>我们理想中的模型：当实际类别<span class="math inline">\(y=1\)</span>，<span class="math inline">\(\hat{p}\)</span>值越大越好；当实际类别<span class="math inline">\(y=0\)</span>，<span class="math inline">\(\hat{p}\)</span>值越小越好。因此我们使用如下损失函数：</p><p>当实际类别<span class="math inline">\(y=1\)</span>时，<span class="math inline">\(c(\theta)=-log(\hat{p})\)</span>；当实际类别<span class="math inline">\(y=0\)</span>是，<span class="math inline">\(c(\theta)=-log(1-\hat{p})\)</span>。即<span class="math inline">\(c(\theta)=-ylog(\hat{p})-(1-y)log(1-\hat{p})\)</span>。</p><p>整个训练集上的损失用平均值衡量：</p><p><span class="math inline">\(J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y^{i}log(\hat{p}^{i})+(1-y^{i})log(1-\hat{p}^{i})]\)</span></p><p>该损失函数不存在封闭式解法，使用梯度下降的办法需要计算损失函数关于<span class="math inline">\(\theta_j\)</span>的导数：</p><p>利用链式求导法则<span class="math inline">\(\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(\sigma(\theta^Tx^i)-y^i)x_j^i\)</span>，<span class="math inline">\(\frac{\partial}{\partial\theta}J(\theta)=\frac{1}{m}X^T(X\theta-y)\)</span></p><p>再求一次导<span class="math inline">\(\frac{\partial^2}{\partial\theta_j^2}J(\theta)=\frac{1}{m}\sum_{i=1}\sigma(\theta^Tx^i)(1-\sigma(\theta^Tx^i)(x_j^i)^2\)</span>，显然<span class="math inline">\(\frac{\partial^2}{\partial\theta_j^2}J(\theta)&gt;0\)</span>，因此损失函数为凸函数，存在全局最优解且能被梯度下降方法找到。</p><h2><span id="5-3-jue-ce-bian-jie">5.3 决策边界</span><a href="#5-3-jue-ce-bian-jie" class="header-anchor">#</a></h2><p>鸢尾花数据集介绍：150个鸢尾花的萼片和花瓣的长度、宽度，鸢尾花共有三个种类Iris setosa, Iris versicolor, 和Iris virginica。简写为se,ve,vi。</p><p>scikit-learn带有一些小的标准数据集，不需要从某些外部网站下载任何文件，可以使用相应函数加载他们。鸢尾花数据集属于其中之一。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">iris = datasets.load_iris()</span><br></pre></td></tr></table></figure><p>第二章提到由sklearn加载返回的数据集一般都是相似的字典结构，我们一般先使用'DESCR'来了解数据集然后使用'data','target'获取数据及其label。尝试建立一个仅基于花瓣宽度特征来检测是否为vi鸢尾花的分类器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = iris[&quot;data&quot;][:, 3:]  # 以二维的方式取出petal width</span><br><span class="line">y = (iris[&quot;target&quot;] == 2).astype(np.int)  # 1 if Iris virginica, else 0</span><br></pre></td></tr></table></figure><p>使用sklearn自带的LogisticRegression训练分类器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import  LogisticRegression</span><br><span class="line"></span><br><span class="line">#--------训练--------</span><br><span class="line">log_reg = LogisticRegression(random_state=42)</span><br><span class="line">log_reg.fit(X, y)</span><br><span class="line">#--------预测--------</span><br><span class="line">X_new = np.linspace(0, 3, 1000).reshape(-1, 1)# reshape为二维数组，只给定列数，numpy自动计算出行数</span><br><span class="line">y_proba = log_reg.predict_proba(X_new)#二维数组，第一列表示为正类的概率，第二列表示为负类的概率</span><br><span class="line"></span><br><span class="line">plt.plot(X_new, y_proba[:, 1], &quot;g-&quot;, linewidth=2, label=&quot;Iris virginica&quot;)</span><br><span class="line">plt.plot(X_new, y_proba[:, 0], &quot;b--&quot;, linewidth=2, label=&quot;Not Iris virginica&quot;)</span><br></pre></td></tr></table></figure><figure><img src="https://myblogdata-1300038172.cos.ap-shanghai.myqcloud.com/images/hands_on_ml_tf/4_5.png" alt="logi_decision_bound"><figcaption>logi_decision_bound</figcaption></figure><p>我们可以看到大约在1.6cm处为它的决策边界，花瓣宽度大于1.6的为vi,否则不是。书上还将花瓣宽度和长度两者作为特征训练了一个分类器，原理类似，不再赘述。</p><p>类似于其他回归操作，logistic regression同样可使用正则化，sklearn默认使用l2。</p><h1><span id="6-softmax-regression">6、Softmax Regression</span><a href="#6-softmax-regression" class="header-anchor">#</a></h1><p>Logistic Regression可推广为多类识别，这成为Softmax Regression。同之前，先介绍Softmax Regression办法如何通过评估概率进行分类预测，然后探究其训练方法。最后为经典数据集-鸢尾花数据集构造分类器。</p><p>评估概率：</p><p>首先为每一类别计算其加权和，然后利用softmax函数将加权和转换为各个类别的概率。</p><p>计算加权和的办法类似Logistic Regression, <span class="math inline">\(s_k(x)={\theta^{(k)}}^Tx\)</span>。用矩阵<span class="math inline">\(\Theta\)</span>表示Softmax Regression的所有参数，矩阵维度为(K,n), k表示类别数，n表示特征数，第k行<span class="math inline">\({\theta^{(k)}}^T\)</span>表示第k类的参数。</p><p>在得到各个类别的分数，即加权和后，利用softmax函数评估各个类别的概率，softmax函数计算每一类别分数的指数，然后进行标准化。<span class="math inline">\(\hat{p}_k=\sigma(s(x))_k=\frac{exp(s_k(x))}{\sum_{j=1}^Kexp(s_j(x))}\)</span>。预测类别<span class="math inline">\(\hat{y}=\arg\max_k\hat{p}_k\)</span>。</p><p>损失函数：</p><p>交叉熵：<span class="math inline">\(J(\Theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^Ky_k^ilog(\hat{p}_k^i)\)</span></p><p>当K=2时，即为Logistic Regression的损失函数。<span class="math inline">\(\frac{\partial}{\partial\theta_j^k}J(\theta)=-\frac{1}{m}\sum_{i=1}^my_k^ix_j^i(1-\hat{p}_k^i)\)</span>。</p><p><span class="math inline">\(\frac{\partial}{\partial\theta^k}J(\theta)=-\frac{1}{m}\sum_{i=1}^my_k^ix^i(1-\hat{p}_k^i)\)</span></p><p>利用Softmax回归构造多分类器，同样使用sklearn自带的LogisticRegression, 只需要再传入参数multi_class=&quot;multi_nomial&quot;，将花瓣的长度和宽度作为训练特征训练多分类器，C控制正则化强度，越大强度越小，<span class="math inline">\(c=\frac{1}{\alpha}\)</span>，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = iris[&quot;data&quot;][:, (2, 3)]  # petal length, petal width</span><br><span class="line">y = iris[&quot;target&quot;]</span><br><span class="line"></span><br><span class="line">softmax_reg = LogisticRegression(multi_class=&quot;multinomial&quot;,solver=&quot;lbfgs&quot;, C=10, random_state=42)</span><br><span class="line">softmax_reg.fit(X, y)</span><br></pre></td></tr></table></figure><p>训练后可进行预测：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">softmax_reg.predict([[5, 2]])</span><br><span class="line">softmax_reg.predict_proba([[5, 2]])</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2020/02/14/hands_on_sklearn_tf_ch3/" rel="next"><i class="fa fa-chevron-left"></i></a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/02/20/hands_on_sklearn_tf_ch5/" rel="prev"><i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">tiny</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">2</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-text">第四章 模型训练#</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-text">1、线性回归#</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">1.1、Normal Function#</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">1.2、梯度下降#</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#null"><span class="nav-text">1.2.1、批量梯度下降#</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#null"><span class="nav-text">1.2.2、随机梯度下降#</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#null"><span class="nav-text">1.2.3、小批量梯度下降#</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-text">2、多项式回归#</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-text">3、学习曲线#</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-text">4、正则化模型#</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">4.1 Ridge Regression#</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">4.2 Lasso Regression#</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">4.3 Elastic Net#</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">4.4 提前停止#</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-text">5、Logistic Regression#</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">5.1 评估概率#</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">5.2 损失函数#</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#null"><span class="nav-text">5.3 决策边界#</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-text">6、Softmax Regression#</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">tiny</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>